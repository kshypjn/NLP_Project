{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ast\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "IMG_DIR = Path(\"sandbox/train_sample\")  \n",
    "\n",
    "MODEL_PATH = Path(\"resnet50_NLP_attributes_final.pth\")\n",
    "\n",
    "OUTPUT_CSV = Path(\"predicted_attributes_final.csv\")\n",
    "\n",
    "NUM_ATTRIBUTES = 294\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageOnlyDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.img_files = list(self.img_dir.glob(\"*.jpg\"))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_files[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, img_path.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "dataset = ImageOnlyDataset(IMG_DIR, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kashyapj/Desktop/NLP_Project/env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/kashyapj/Desktop/NLP_Project/env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: resnet50_NLP_attributes_final.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=294, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=False)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, NUM_ATTRIBUTES)\n",
    "\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting attributes: 100%|██████████| 125/125 [12:33<00:00,  6.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted attributes saved to predicted_attributes_final.csv\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "with torch.no_grad():\n",
    "    for imgs, img_ids in tqdm(loader, desc=\"Predicting attributes\"):\n",
    "        imgs = imgs.to(device)\n",
    "        outputs = model(imgs) \n",
    "        probs = torch.sigmoid(outputs)    \n",
    "        preds = (probs > 0.5).int().cpu().numpy()  \n",
    "\n",
    "        for image_id, pred_vector in zip(img_ids, preds):\n",
    "            results.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"attributes\": list(pred_vector)\n",
    "            })\n",
    "\n",
    "df_preds = pd.DataFrame(results)\n",
    "df_preds.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Predicted attributes saved to {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (2000, 294) (2000, 294)\n",
      "Unique true labels: [0 1]\n",
      "Unique pred labels: [0 1]\n",
      "Macro Precision: 0.2605\n",
      "Macro Recall:    0.1511\n",
      "Macro F1:        0.1731\n",
      "Micro F1:        0.6484\n",
      "\n",
      "Per-attribute classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      attr_0       0.84      0.82      0.83       239\n",
      "      attr_1       0.00      0.00      0.00         8\n",
      "      attr_2       0.00      0.00      0.00        27\n",
      "      attr_3       0.00      0.00      0.00         4\n",
      "      attr_4       0.00      0.00      0.00         3\n",
      "      attr_5       0.00      0.00      0.00         6\n",
      "      attr_6       0.00      0.00      0.00         2\n",
      "      attr_7       0.00      0.00      0.00        10\n",
      "      attr_8       1.00      0.39      0.56        49\n",
      "      attr_9       0.00      0.00      0.00        16\n",
      "     attr_10       0.90      0.31      0.46        61\n",
      "     attr_11       0.94      0.70      0.80        92\n",
      "     attr_12       0.00      0.00      0.00        10\n",
      "     attr_13       0.00      0.00      0.00        19\n",
      "     attr_14       0.00      0.00      0.00        11\n",
      "     attr_15       0.00      0.00      0.00         3\n",
      "     attr_16       0.00      0.00      0.00        17\n",
      "     attr_17       0.81      0.70      0.75       115\n",
      "     attr_18       0.00      0.00      0.00         2\n",
      "     attr_19       0.00      0.00      0.00         5\n",
      "     attr_20       0.70      0.56      0.62        34\n",
      "     attr_21       1.00      0.12      0.21        25\n",
      "     attr_22       0.00      0.00      0.00        20\n",
      "     attr_23       0.00      0.00      0.00         1\n",
      "     attr_24       0.00      0.00      0.00         4\n",
      "     attr_25       0.00      0.00      0.00         0\n",
      "     attr_26       0.00      0.00      0.00         1\n",
      "     attr_27       0.00      0.00      0.00         5\n",
      "     attr_28       0.00      0.00      0.00         2\n",
      "     attr_29       0.00      0.00      0.00        12\n",
      "     attr_30       0.00      0.00      0.00         6\n",
      "     attr_31       0.00      0.00      0.00         5\n",
      "     attr_32       0.00      0.00      0.00         2\n",
      "     attr_33       0.50      0.12      0.19        17\n",
      "     attr_34       0.00      0.00      0.00         1\n",
      "     attr_35       0.00      0.00      0.00         4\n",
      "     attr_36       0.88      0.83      0.85       170\n",
      "     attr_37       0.00      0.00      0.00        11\n",
      "     attr_38       0.97      0.49      0.65        69\n",
      "     attr_39       0.00      0.00      0.00         7\n",
      "     attr_40       0.00      0.00      0.00         4\n",
      "     attr_41       0.00      0.00      0.00         0\n",
      "     attr_42       0.00      0.00      0.00        12\n",
      "     attr_43       0.00      0.00      0.00         2\n",
      "     attr_44       0.00      0.00      0.00        11\n",
      "     attr_45       0.00      0.00      0.00         2\n",
      "     attr_46       0.00      0.00      0.00        12\n",
      "     attr_47       0.00      0.00      0.00         3\n",
      "     attr_48       0.00      0.00      0.00         2\n",
      "     attr_49       0.00      0.00      0.00         8\n",
      "     attr_50       0.87      0.75      0.80        63\n",
      "     attr_51       0.00      0.00      0.00         2\n",
      "     attr_52       0.00      0.00      0.00        12\n",
      "     attr_53       0.00      0.00      0.00         0\n",
      "     attr_54       0.00      0.00      0.00         3\n",
      "     attr_55       0.00      0.00      0.00         0\n",
      "     attr_56       0.00      0.00      0.00         1\n",
      "     attr_57       0.00      0.00      0.00        11\n",
      "     attr_58       0.00      0.00      0.00         2\n",
      "     attr_59       0.00      0.00      0.00         3\n",
      "     attr_60       0.00      0.00      0.00         8\n",
      "     attr_61       0.00      0.00      0.00         1\n",
      "     attr_62       0.00      0.00      0.00         5\n",
      "     attr_63       0.00      0.00      0.00         1\n",
      "     attr_64       0.00      0.00      0.00         3\n",
      "     attr_65       0.00      0.00      0.00         9\n",
      "     attr_66       0.00      0.00      0.00         0\n",
      "     attr_67       0.00      0.00      0.00        10\n",
      "     attr_68       0.82      0.60      0.69        75\n",
      "     attr_69       0.00      0.00      0.00         7\n",
      "     attr_70       0.00      0.00      0.00         1\n",
      "     attr_71       0.00      0.00      0.00         8\n",
      "     attr_72       0.00      0.00      0.00         2\n",
      "     attr_73       0.00      0.00      0.00         0\n",
      "     attr_74       0.00      0.00      0.00         2\n",
      "     attr_75       0.00      0.00      0.00         1\n",
      "     attr_76       0.00      0.00      0.00         0\n",
      "     attr_77       0.00      0.00      0.00        18\n",
      "     attr_78       0.00      0.00      0.00         2\n",
      "     attr_79       1.00      0.04      0.08        24\n",
      "     attr_80       0.00      0.00      0.00         6\n",
      "     attr_81       0.00      0.00      0.00        10\n",
      "     attr_82       0.00      0.00      0.00         5\n",
      "     attr_83       0.00      0.00      0.00         2\n",
      "     attr_84       0.00      0.00      0.00         2\n",
      "     attr_85       0.00      0.00      0.00         3\n",
      "     attr_86       0.00      0.00      0.00         6\n",
      "     attr_87       0.00      0.00      0.00         0\n",
      "     attr_88       0.00      0.00      0.00         3\n",
      "     attr_89       0.00      0.00      0.00         0\n",
      "     attr_90       0.00      0.00      0.00         2\n",
      "     attr_91       0.00      0.00      0.00         1\n",
      "     attr_92       0.00      0.00      0.00         8\n",
      "     attr_93       0.00      0.00      0.00         2\n",
      "     attr_94       0.00      0.00      0.00         3\n",
      "     attr_95       1.00      0.04      0.08        23\n",
      "     attr_96       0.00      0.00      0.00         6\n",
      "     attr_97       0.00      0.00      0.00         5\n",
      "     attr_98       0.00      0.00      0.00        24\n",
      "     attr_99       0.00      0.00      0.00         0\n",
      "    attr_100       0.00      0.00      0.00        10\n",
      "    attr_101       0.96      0.38      0.55        68\n",
      "    attr_102       0.84      0.66      0.74        87\n",
      "    attr_103       1.00      0.05      0.09        22\n",
      "    attr_104       0.00      0.00      0.00        14\n",
      "    attr_105       0.00      0.00      0.00         7\n",
      "    attr_106       0.78      0.58      0.67        55\n",
      "    attr_107       0.00      0.00      0.00         2\n",
      "    attr_108       0.91      0.84      0.87       215\n",
      "    attr_109       0.00      0.00      0.00        12\n",
      "    attr_110       0.00      0.00      0.00         4\n",
      "    attr_111       0.00      0.00      0.00         9\n",
      "    attr_112       1.00      0.07      0.13        44\n",
      "    attr_113       0.80      0.51      0.62        63\n",
      "    attr_114       0.78      0.63      0.70        97\n",
      "    attr_115       0.96      0.97      0.97      1704\n",
      "    attr_116       0.00      0.00      0.00        12\n",
      "    attr_117       0.96      0.33      0.49        72\n",
      "    attr_118       0.82      0.15      0.25        60\n",
      "    attr_119       0.94      0.68      0.78       151\n",
      "    attr_120       1.00      0.04      0.07        27\n",
      "    attr_121       1.00      0.56      0.72        25\n",
      "    attr_122       0.00      0.00      0.00         7\n",
      "    attr_123       0.00      0.00      0.00        32\n",
      "    attr_124       0.00      0.00      0.00         2\n",
      "    attr_125       0.00      0.00      0.00        14\n",
      "    attr_126       1.00      0.14      0.24        43\n",
      "    attr_127       0.87      0.83      0.85       157\n",
      "    attr_128       0.88      0.71      0.78       303\n",
      "    attr_129       0.93      0.61      0.74       181\n",
      "    attr_130       0.00      0.00      0.00        14\n",
      "    attr_131       0.00      0.00      0.00         3\n",
      "    attr_132       0.00      0.00      0.00        23\n",
      "    attr_133       0.00      0.00      0.00        30\n",
      "    attr_134       0.00      0.00      0.00         6\n",
      "    attr_135       0.90      0.82      0.86       505\n",
      "    attr_136       0.89      0.92      0.90       757\n",
      "    attr_137       0.80      0.70      0.75       201\n",
      "    attr_138       0.00      0.00      0.00        21\n",
      "    attr_139       0.00      0.00      0.00        18\n",
      "    attr_140       1.00      0.47      0.63        43\n",
      "    attr_141       0.92      0.77      0.84       459\n",
      "    attr_142       0.87      0.88      0.88       904\n",
      "    attr_143       0.65      0.59      0.62       166\n",
      "    attr_144       0.00      0.00      0.00         4\n",
      "    attr_145       0.92      0.87      0.90       759\n",
      "    attr_146       0.92      0.88      0.90       685\n",
      "    attr_147       0.87      0.83      0.85       407\n",
      "    attr_148       0.96      0.35      0.51       138\n",
      "    attr_149       0.89      0.86      0.87       378\n",
      "    attr_150       0.87      0.86      0.86       191\n",
      "    attr_151       0.88      0.49      0.63       120\n",
      "    attr_152       1.00      0.13      0.23        70\n",
      "    attr_153       0.95      0.52      0.68       101\n",
      "    attr_154       0.93      0.89      0.91       400\n",
      "    attr_155       0.96      0.83      0.90       230\n",
      "    attr_156       0.00      0.00      0.00         7\n",
      "    attr_157       1.00      0.81      0.90       296\n",
      "    attr_158       1.00      0.08      0.14        78\n",
      "    attr_159       0.98      0.58      0.73       161\n",
      "    attr_160       0.95      0.93      0.94       720\n",
      "    attr_161       0.00      0.00      0.00         2\n",
      "    attr_162       1.00      0.15      0.26        75\n",
      "    attr_163       0.95      0.70      0.81       222\n",
      "    attr_164       0.00      0.00      0.00         8\n",
      "    attr_165       0.00      0.00      0.00         1\n",
      "    attr_166       0.00      0.00      0.00        24\n",
      "    attr_167       0.00      0.00      0.00        11\n",
      "    attr_168       0.00      0.00      0.00         9\n",
      "    attr_169       0.00      0.00      0.00         4\n",
      "    attr_170       0.00      0.00      0.00        29\n",
      "    attr_171       0.00      0.00      0.00         0\n",
      "    attr_172       0.00      0.00      0.00         1\n",
      "    attr_173       0.00      0.00      0.00        10\n",
      "    attr_174       0.77      0.61      0.68        96\n",
      "    attr_175       0.00      0.00      0.00        27\n",
      "    attr_176       0.73      0.24      0.36        34\n",
      "    attr_177       1.00      0.04      0.07        26\n",
      "    attr_178       0.00      0.00      0.00        19\n",
      "    attr_179       0.00      0.00      0.00        43\n",
      "    attr_180       0.63      0.73      0.68        37\n",
      "    attr_181       0.89      0.21      0.34        38\n",
      "    attr_182       0.91      0.74      0.81       257\n",
      "    attr_183       0.95      0.16      0.28       129\n",
      "    attr_184       0.00      0.00      0.00        25\n",
      "    attr_185       0.93      0.39      0.55       147\n",
      "    attr_186       0.92      0.22      0.36        49\n",
      "    attr_187       1.00      0.01      0.03        77\n",
      "    attr_188       0.00      0.00      0.00        12\n",
      "    attr_189       0.00      0.00      0.00        56\n",
      "    attr_190       1.00      0.21      0.34       111\n",
      "    attr_191       0.00      0.00      0.00        33\n",
      "    attr_192       0.00      0.00      0.00        33\n",
      "    attr_193       0.00      0.00      0.00         4\n",
      "    attr_194       0.00      0.00      0.00        25\n",
      "    attr_195       0.00      0.00      0.00         2\n",
      "    attr_196       0.00      0.00      0.00         2\n",
      "    attr_197       0.00      0.00      0.00        31\n",
      "    attr_198       0.00      0.00      0.00        27\n",
      "    attr_199       0.00      0.00      0.00        15\n",
      "    attr_200       0.92      0.81      0.86       102\n",
      "    attr_201       0.00      0.00      0.00        11\n",
      "    attr_202       0.00      0.00      0.00        32\n",
      "    attr_203       0.72      0.52      0.60        25\n",
      "    attr_204       0.93      0.94      0.94       921\n",
      "    attr_205       0.84      0.53      0.65        79\n",
      "    attr_206       1.00      0.13      0.24        30\n",
      "    attr_207       1.00      0.41      0.58        54\n",
      "    attr_208       0.00      0.00      0.00         6\n",
      "    attr_209       0.00      0.00      0.00        19\n",
      "    attr_210       0.00      0.00      0.00         8\n",
      "    attr_211       0.00      0.00      0.00        11\n",
      "    attr_212       0.00      0.00      0.00         8\n",
      "    attr_213       0.00      0.00      0.00        18\n",
      "    attr_214       0.00      0.00      0.00        17\n",
      "    attr_215       0.00      0.00      0.00        11\n",
      "    attr_216       1.00      0.06      0.12        16\n",
      "    attr_217       0.00      0.00      0.00        11\n",
      "    attr_218       0.96      0.44      0.61       169\n",
      "    attr_219       0.84      0.75      0.79       146\n",
      "    attr_220       1.00      0.04      0.08        23\n",
      "    attr_221       0.00      0.00      0.00        20\n",
      "    attr_222       0.90      0.61      0.73       129\n",
      "    attr_223       0.76      0.76      0.76       164\n",
      "    attr_224       0.81      0.37      0.51       119\n",
      "    attr_225       0.95      0.77      0.85       437\n",
      "    attr_226       0.00      0.00      0.00        18\n",
      "    attr_227       0.00      0.00      0.00        15\n",
      "    attr_228       0.83      0.10      0.18        49\n",
      "    attr_229       0.88      0.89      0.88       742\n",
      "    attr_230       0.84      0.93      0.88       402\n",
      "    attr_231       0.00      0.00      0.00         0\n",
      "    attr_232       0.00      0.00      0.00         3\n",
      "    attr_233       0.00      0.00      0.00         1\n",
      "    attr_234       0.95      0.69      0.80       198\n",
      "    attr_235       0.00      0.00      0.00       137\n",
      "    attr_236       0.00      0.00      0.00        10\n",
      "    attr_237       0.00      0.00      0.00        50\n",
      "    attr_238       0.00      0.00      0.00         0\n",
      "    attr_239       0.00      0.00      0.00        33\n",
      "    attr_240       0.00      0.00      0.00         0\n",
      "    attr_241       0.00      0.00      0.00         0\n",
      "    attr_242       0.00      0.00      0.00        29\n",
      "    attr_243       0.00      0.00      0.00         7\n",
      "    attr_244       0.00      0.00      0.00         3\n",
      "    attr_245       0.00      0.00      0.00         1\n",
      "    attr_246       0.00      0.00      0.00         0\n",
      "    attr_247       0.00      0.00      0.00         1\n",
      "    attr_248       0.00      0.00      0.00      1837\n",
      "    attr_249       0.00      0.00      0.00        17\n",
      "    attr_250       0.00      0.00      0.00        38\n",
      "    attr_251       0.00      0.00      0.00       163\n",
      "    attr_252       0.00      0.00      0.00         6\n",
      "    attr_253       0.00      0.00      0.00        25\n",
      "    attr_254       0.00      0.00      0.00       330\n",
      "    attr_255       0.00      0.00      0.00        57\n",
      "    attr_256       0.00      0.00      0.00         9\n",
      "    attr_257       0.00      0.00      0.00       107\n",
      "    attr_258       0.00      0.00      0.00       192\n",
      "    attr_259       0.00      0.00      0.00         8\n",
      "    attr_260       0.00      0.00      0.00        26\n",
      "    attr_261       0.00      0.00      0.00        33\n",
      "    attr_262       0.00      0.00      0.00        35\n",
      "    attr_263       0.00      0.00      0.00         7\n",
      "    attr_264       0.00      0.00      0.00       417\n",
      "    attr_265       0.00      0.00      0.00       106\n",
      "    attr_266       0.00      0.00      0.00        92\n",
      "    attr_267       0.00      0.00      0.00        33\n",
      "    attr_268       0.00      0.00      0.00        19\n",
      "    attr_269       0.00      0.00      0.00      1178\n",
      "    attr_270       0.00      0.00      0.00      1645\n",
      "    attr_271       0.00      0.00      0.00        66\n",
      "    attr_272       0.00      0.00      0.00        27\n",
      "    attr_273       0.00      0.00      0.00        73\n",
      "    attr_274       0.00      0.00      0.00        10\n",
      "    attr_275       0.00      0.00      0.00        81\n",
      "    attr_276       0.00      0.00      0.00        40\n",
      "    attr_277       0.00      0.00      0.00        12\n",
      "    attr_278       0.00      0.00      0.00       150\n",
      "    attr_279       0.00      0.00      0.00        55\n",
      "    attr_280       0.00      0.00      0.00        11\n",
      "    attr_281       0.00      0.00      0.00       110\n",
      "    attr_282       0.00      0.00      0.00         5\n",
      "    attr_283       0.00      0.00      0.00         4\n",
      "    attr_284       0.00      0.00      0.00         7\n",
      "    attr_285       0.00      0.00      0.00         4\n",
      "    attr_286       0.00      0.00      0.00         7\n",
      "    attr_287       0.00      0.00      0.00         1\n",
      "    attr_288       0.00      0.00      0.00         5\n",
      "    attr_289       0.00      0.00      0.00         1\n",
      "    attr_290       0.00      0.00      0.00         3\n",
      "    attr_291       0.00      0.00      0.00         2\n",
      "    attr_292       0.00      0.00      0.00        22\n",
      "    attr_293       0.00      0.00      0.00        21\n",
      "\n",
      "   micro avg       0.90      0.51      0.65     26063\n",
      "   macro avg       0.26      0.15      0.17     26063\n",
      "weighted avg       0.60      0.51      0.53     26063\n",
      " samples avg       0.88      0.50      0.63     26063\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "def parse_np_int_list(s):\n",
    "   \n",
    "    digits = re.findall(r\"np\\.int32\\(\\s*(\\d+)\\s*\\)\", s)\n",
    "    if digits:\n",
    "        return [int(x) for x in digits]\n",
    "   \n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "def parse_float_list(s):\n",
    "    \n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "\n",
    "gt_df = pd.read_csv(\n",
    "    \"sandbox/train_attribute_data.csv\",\n",
    "    converters={\"attributes\": parse_np_int_list}\n",
    ")\n",
    "\n",
    "\n",
    "pred_df = pd.read_csv(\n",
    "    \"predicted_attributes_final.csv\",\n",
    "    converters={\n",
    "        \"attributes\": parse_np_int_list,\n",
    "       \n",
    "    }\n",
    ")\n",
    "\n",
    "merged = pd.merge(\n",
    "    gt_df.rename(columns={\"attributes\":\"attributes_true\"}),\n",
    "    pred_df.rename(columns={\"attributes\":\"attributes_pred\"}),\n",
    "    on=\"image_id\"\n",
    ")\n",
    "\n",
    "y_true = np.vstack(merged[\"attributes_true\"].values)  \n",
    "y_pred = np.vstack(merged[\"attributes_pred\"].values)\n",
    "\n",
    "\n",
    "print(\"Shapes:\", y_true.shape, y_pred.shape)\n",
    "print(\"Unique true labels:\", np.unique(y_true))\n",
    "print(\"Unique pred labels:\", np.unique(y_pred))\n",
    "macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "micro_f1 = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "macro_p  = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "macro_r  = recall_score   (y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(f\"Macro Precision: {macro_p:.4f}\")\n",
    "print(f\"Macro Recall:    {macro_r:.4f}\")\n",
    "print(f\"Macro F1:        {macro_f1:.4f}\")\n",
    "print(f\"Micro F1:        {micro_f1:.4f}\\n\")\n",
    "\n",
    "print(\"Per-attribute classification report:\")\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    zero_division=0,\n",
    "    target_names=[f\"attr_{i}\" for i in range(y_true.shape[1])]\n",
    "))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 rarest attributes:\n",
      " attr_id           name  frequency\n",
      "      87       raincoat          0\n",
      "     246      snakeskin          0\n",
      "      99     cheongsams          0\n",
      "      53 cargo (shorts)          0\n",
      "      55    boardshorts          0\n",
      "     238        feather          0\n",
      "      25   mao (jacket)          0\n",
      "      66  cargo (skirt)          0\n",
      "      41       culottes          0\n",
      "     240           bone          0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import json\n",
    "\n",
    "GT_CSV      = \"predicted_attributes_final.csv\"\n",
    "LABELS_JSON = \"sandbox/label_descriptions.json\"\n",
    "def parse_np_int_list(s):\n",
    "       \n",
    "    digits = re.findall(r\"np\\.int32\\(\\s*(\\d+)\\s*\\)\", s)\n",
    "    if digits:\n",
    "        return [int(x) for x in digits]\n",
    "   \n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "def parse_float_list(s):\n",
    "    \n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "gt_df = pd.read_csv(\n",
    "    \"predicted_attributes_final.csv\",\n",
    "    converters={\n",
    "        \"attributes\": parse_np_int_list,\n",
    "       \n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "with open(LABELS_JSON) as f:\n",
    "    label_data = json.load(f)\n",
    "attr_names = [a[\"name\"] for a in label_data[\"attributes\"]]\n",
    "\n",
    "counts = np.vstack(gt[\"attributes\"].values).sum(axis=0)\n",
    "df_freq = pd.DataFrame({\n",
    "    \"attr_id\":   np.arange(len(counts)),\n",
    "    \"name\":      attr_names,\n",
    "    \"frequency\": counts\n",
    "})\n",
    "\n",
    "df_rare = df_freq.sort_values(\"frequency\").reset_index(drop=True)\n",
    "\n",
    "K = 10\n",
    "print(f\"Top {K} rarest attributes:\")\n",
    "print(df_rare.head(K).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
