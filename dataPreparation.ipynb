{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations CSV: sandbox/train_sample_annotations.csv\n",
      "Labels JSON: sandbox/label_descriptions.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths â€” adjust them as needed for your environment\n",
    "sandbox_dir = Path(\"sandbox\")\n",
    "annotations_path = sandbox_dir / \"train_sample_annotations.csv\"\n",
    "labels_path = sandbox_dir / \"label_descriptions.json\"\n",
    "\n",
    "# Print to ensure they exist\n",
    "print(\"Annotations CSV:\", annotations_path)\n",
    "print(\"Labels JSON:\", labels_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotation rows: 14765\n",
      "Number of attributes defined: 294\n"
     ]
    }
   ],
   "source": [
    "# Load the annotations CSV (which includes columns like ImageId, AttributesIds, etc.)\n",
    "ann_df = pd.read_csv(annotations_path)\n",
    "print(\"Total annotation rows:\", len(ann_df))\n",
    "\n",
    "\n",
    "with open(labels_path, \"r\") as f:\n",
    "    label_data = json.load(f)\n",
    "\n",
    "attributes_list = label_data[\"attributes\"]\n",
    "num_attributes = len(attributes_list)\n",
    "print(\"Number of attributes defined:\", num_attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute mapping (first 5): {0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n"
     ]
    }
   ],
   "source": [
    "# Build a mapping: attribute id --> index in our vector\n",
    "attr_ids = [attr[\"id\"] for attr in attributes_list]\n",
    "attr_to_index = {attr_id: idx for idx, attr_id in enumerate(attr_ids)}\n",
    "print(\"Attribute mapping (first 5):\", dict(list(attr_to_index.items())[:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           image_id  \\\n",
      "0  0000fe7c9191fba733c8a69cfaf962b7   \n",
      "1  001a66b16b12f12dc45e2bba40e04683   \n",
      "2  00382465705798a714595f1d043a24e6   \n",
      "3  003ad8a37d2190bd944a8968fb0906e2   \n",
      "4  0040e5863c5e6197cd264509bc2fbb1c   \n",
      "\n",
      "                                          attributes  \n",
      "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "# Group the annotations by ImageId and create a binary vector for each image\n",
    "training_data = []\n",
    "\n",
    "# Group annotations by image id\n",
    "grouped = ann_df.groupby(\"ImageId\")\n",
    "\n",
    "for img_id, group in grouped:\n",
    "    vec = np.zeros(num_attributes, dtype=int)\n",
    "    # Iterate through every annotation row for the image\n",
    "    for _, row in group.iterrows():\n",
    "        raw_attr = str(row[\"AttributesIds\"])  # Ensure it's a string\n",
    "        # Split by comma; some rows might have multiple attribute ids\n",
    "        for a in raw_attr.split(\",\"):\n",
    "            a = a.strip()\n",
    "            if a.isdigit():\n",
    "                a_int = int(a)\n",
    "                # Set the corresponding position if a_int exists in our mapping\n",
    "                if a_int in attr_to_index:\n",
    "                    vec[attr_to_index[a_int]] = 1\n",
    "    training_data.append({\"image_id\": img_id, \"attributes\": vec.tolist()})\n",
    "\n",
    "# Create a DataFrame\n",
    "train_attr_df = pd.DataFrame(training_data)\n",
    "print(train_attr_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-label attribute training data saved to: sandbox/train_attribute_data.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = sandbox_dir / \"train_attribute_data.csv\"\n",
    "train_attr_df.to_csv(output_path, index=False)\n",
    "print(f\"Multi-label attribute training data saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
