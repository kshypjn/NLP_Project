{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":329006,"sourceType":"datasetVersion","datasetId":139630},{"sourceId":11470981,"sourceType":"datasetVersion","datasetId":7188733}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch, json, gc, os, re\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nimport pandas as pd\nUSE_FP16       = True\nMAX_NEW_TOKENS = 150\nBATCH_SIZE     = 3\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:49:19.666301Z","iopub.execute_input":"2025-04-20T10:49:19.666555Z","iopub.status.idle":"2025-04-20T10:49:45.835107Z","shell.execute_reply.started":"2025-04-20T10:49:19.666530Z","shell.execute_reply":"2025-04-20T10:49:45.834533Z"}},"outputs":[{"name":"stderr","text":"2025-04-20 10:49:31.666204: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745146171.899154      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745146171.967323      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nBASE_DIR        = Path(\"/kaggle/input/indofashion-nlp\")\nIMG_DIR         = BASE_DIR/\"train_sand\"/\"train_sand\"\nMETA_JSON_PATH  = BASE_DIR/\"indofashion_metadata.json\"\nOUTPUT_CSV      = \"LLaVADescriptionsIndofashion.csv\"\nCHECKPOINT_FILE = \"llava_indofashion_checkpoint.json\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:49:48.312006Z","iopub.execute_input":"2025-04-20T10:49:48.312833Z","iopub.status.idle":"2025-04-20T10:49:48.404081Z","shell.execute_reply.started":"2025-04-20T10:49:48.312809Z","shell.execute_reply":"2025-04-20T10:49:48.403205Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"meta_df = pd.read_json(META_JSON_PATH, lines=True)\nprint(\"Available columns:\", meta_df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:49:50.567833Z","iopub.execute_input":"2025-04-20T10:49:50.568101Z","iopub.status.idle":"2025-04-20T10:49:50.617567Z","shell.execute_reply.started":"2025-04-20T10:49:50.568081Z","shell.execute_reply":"2025-04-20T10:49:50.616756Z"}},"outputs":[{"name":"stdout","text":"Available columns: ['image_id', 'class_label', 'brand', 'product_title']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"meta_df = pd.read_json(META_JSON_PATH, lines=True)\nmeta_df = meta_df[~meta_df[\"class_label\"]\n                  .isin([\"petticoats\",\"mojaris_men\",\"mojaris_women\",\"dupattas\"])]\nmeta_df = meta_df.set_index(\"image_id\", drop=False)\nmeta_df.rename(\n    columns={\n        \"product_title\": \"title\",\n    },\n    inplace=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:49:52.246727Z","iopub.execute_input":"2025-04-20T10:49:52.247083Z","iopub.status.idle":"2025-04-20T10:49:52.278286Z","shell.execute_reply.started":"2025-04-20T10:49:52.247058Z","shell.execute_reply":"2025-04-20T10:49:52.277546Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def build_prompt_llava_indofashion(row):\n    item_cls = row[\"class_label\"].strip().lower()\n    brand    = row[\"brand\"].strip()\n    title    = row[\"title\"].strip().rstrip(\".\")\n\n    words = title.split()\n    if len(words) > 12:\n        title = \" \".join(words[:12]) + \"…\"\n\n    title = title.replace('\"',\"\").replace(\"“\",\"\").replace(\"”\",\"\")\n    parts = []\n    if item_cls: parts.append(item_cls)\n    if brand:    parts.append(f\"by {brand}\")\n    if title:    parts.append(f'titled \"{title}\"')\n    descriptor = \", \".join(parts)\n    return (\n        \"USER: <image>\\n\"\n        \"You’re an e‑commerce copywriter. Describe this garment in vivid detail—\"\n        \"focus on its color, fabric/material, pattern or embroidery, silhouette, \"\n        \"neckline, sleeves or drape style, length, and any special accents or embellishments. \"\n        f\"This is {descriptor}. \"\n        \"Do NOT mention model, person, or background—only describe the garment itself.\\n\"\n        \"ASSISTANT:\"\n    )\n\nmeta_df[\"prompt\"] = meta_df.apply(build_prompt_llava_indofashion, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:49:54.100529Z","iopub.execute_input":"2025-04-20T10:49:54.101297Z","iopub.status.idle":"2025-04-20T10:49:54.139408Z","shell.execute_reply.started":"2025-04-20T10:49:54.101264Z","shell.execute_reply":"2025-04-20T10:49:54.138638Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_checkpoint():\n    if os.path.exists(CHECKPOINT_FILE):\n        return json.load(open(CHECKPOINT_FILE))\n    return {\"processed_ids\": [], \"descriptions\": []}\n\ndef save_checkpoint(cp):\n    json.dump(cp, open(CHECKPOINT_FILE, \"w\"))\n\ndef clean_up():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:49:56.602363Z","iopub.execute_input":"2025-04-20T10:49:56.602669Z","iopub.status.idle":"2025-04-20T10:49:56.607399Z","shell.execute_reply.started":"2025-04-20T10:49:56.602647Z","shell.execute_reply":"2025-04-20T10:49:56.606736Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model_id  = \"llava-hf/llava-1.5-13b-hf\"\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel     = AutoModelForVision2Seq.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16 if USE_FP16 else torch.float32,\n    device_map=\"auto\",\n    low_cpu_mem_usage=True\n)\nmodel.gradient_checkpointing_enable()\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:49:58.367911Z","iopub.execute_input":"2025-04-20T10:49:58.368170Z","iopub.status.idle":"2025-04-20T10:53:08.545153Z","shell.execute_reply.started":"2025-04-20T10:49:58.368152Z","shell.execute_reply":"2025-04-20T10:53:08.544632Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e3dd490ec02425faecf7a99ad342b70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/701 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b95f1ed9626648c5a1b20fe7ec66f779"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/505 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1d7d1c5e28a4d029c8cc4ba74a7dfb7"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bb07a06302844f9be392b089bed4e8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cc14c2e316044e59283f6d4baead03e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.62M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"794cd57882e14b299705317b60e84279"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d465fb2e570a45e1a760a4f8199c6013"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92dd6b73ad9e4981bd3bd5f824fac6e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc17aab5e77d45eda7197713b69e13f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/77.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd21561a5b414f82bbff2c77ff26d521"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa94fcf5134947e1bd332c6cec9ceffb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00006.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f37622832234fc593338e2c2a3abeb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00006.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00573a5c1caf42bb942bee47baaa382b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00006.safetensors:   0%|          | 0.00/2.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00b65f9d6d764ddeb943508832584f20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00006.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"461923804e27481f93218bb84503837d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00006.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2666d7e5c9b84c0d95eba3c35f6dd99d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00006.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ba9160290c84c1fb5c2bca025c584cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e56f24d3f50a4991af19362d254024a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d51c1e5aba004e09b69c10dc89d6aff5"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"LlavaForConditionalGeneration(\n  (vision_tower): CLIPVisionModel(\n    (vision_model): CLIPVisionTransformer(\n      (embeddings): CLIPVisionEmbeddings(\n        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n        (position_embedding): Embedding(577, 1024)\n      )\n      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (encoder): CLIPEncoder(\n        (layers): ModuleList(\n          (0-23): 24 x CLIPEncoderLayer(\n            (self_attn): CLIPSdpaAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (multi_modal_projector): LlavaMultiModalProjector(\n    (linear_1): Linear(in_features=1024, out_features=5120, bias=True)\n    (act): GELUActivation()\n    (linear_2): Linear(in_features=5120, out_features=5120, bias=True)\n  )\n  (language_model): LlamaForCausalLM(\n    (model): LlamaModel(\n      (embed_tokens): Embedding(32064, 5120)\n      (layers): ModuleList(\n        (0-39): 40 x LlamaDecoderLayer(\n          (self_attn): LlamaAttention(\n            (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n            (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n            (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n            (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n          )\n          (mlp): LlamaMLP(\n            (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n            (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n            (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n          (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n        )\n      )\n      (norm): LlamaRMSNorm((5120,), eps=1e-05)\n      (rotary_emb): LlamaRotaryEmbedding()\n    )\n    (lm_head): Linear(in_features=5120, out_features=32064, bias=False)\n  )\n)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def clean_up():\n    torch.cuda.empty_cache()\n    gc.collect()\n\nckpt = load_checkpoint()\nprocessed = set(ckpt[\"processed_ids\"])\nresults = ckpt[\"descriptions\"]\nto_do = [obj_id for obj_id in meta_df.index if obj_id not in processed]\nfor index in tqdm(range(0, len(to_do), BATCH_SIZE)):\n    batch_ids = to_do[index : index + BATCH_SIZE]\n    imgs, prompts = [], []\n    valid_batch_ids = []  # Keep track of IDs with valid images\n    \n    for obj_id in batch_ids:\n        # Look for any of the three file formats\n        img_path = None\n        for ext in [\".jpeg\", \".jpg\", \".png\"]:\n            potential_path = IMG_DIR/f\"{obj_id}{ext}\"\n            if potential_path.exists():\n                img_path = potential_path\n                break\n        \n        if img_path is None:\n            print(f\"Warning: No image found for {obj_id}\")\n            continue\n            \n        img = Image.open(img_path).convert(\"RGB\")\n        img = img.resize((336, 336), Image.LANCZOS)\n        imgs.append(img)\n        prompts.append(meta_df.at[obj_id, \"prompt\"])\n        valid_batch_ids.append(obj_id)  # Only add valid IDs\n        \n    if not imgs:\n        continue  \n        \n    clean_up()\n    inputs = processor(\n        images=imgs,\n        text=prompts,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True\n    ).to(device)\n    \n    with torch.amp.autocast('cuda', enabled=USE_FP16), torch.no_grad():\n        out_ids = model.generate(\n            **inputs,\n            max_new_tokens=MAX_NEW_TOKENS,\n            do_sample=False,\n            use_cache=True,\n\n            min_length=10,  \n            num_return_sequences=1,\n            no_repeat_ngram_size=3,\n            early_stopping=True \n        )\n    \n    outputs = processor.batch_decode(out_ids, skip_special_tokens=True)\n    \n    for obj_id, raw in zip(valid_batch_ids, outputs):\n        desc = raw.split(\"ASSISTANT:\", 1)[-1].strip()\n        if not desc.endswith(('.', '!', '?')):\n            # Try to find the last complete sentence\n            last_sentence_end = max(desc.rfind('.'), desc.rfind('!'), desc.rfind('?'))\n            if last_sentence_end > len(desc) * 0.5:  # Only truncate if we have at least half the text\n                desc = desc[:last_sentence_end + 1]\n            else:\n                desc = desc + \".\"\n        \n        results.append({\"image_id\": obj_id, \"description\": desc})\n        processed.add(obj_id)\n    \n    save_checkpoint({\"processed_ids\": list(processed), \"descriptions\": results})\n    clean_up()\n\npd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\nprint(f\"Descriptions saved - {OUTPUT_CSV}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:53:33.038462Z","iopub.execute_input":"2025-04-20T10:53:33.039217Z","iopub.status.idle":"2025-04-20T15:47:55.576323Z","shell.execute_reply.started":"2025-04-20T10:53:33.039189Z","shell.execute_reply":"2025-04-20T15:47:55.575634Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/667 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\n 27%|██▋       | 178/667 [1:18:41<3:37:04, 26.63s/it]","output_type":"stream"},{"name":"stdout","text":"Warning: No image found for 5846\n","output_type":"stream"},{"name":"stderr","text":" 70%|██████▉   | 465/667 [3:25:23<1:30:20, 26.83s/it]","output_type":"stream"},{"name":"stdout","text":"Warning: No image found for 5810\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▉  | 530/667 [3:54:10<1:00:58, 26.70s/it]","output_type":"stream"},{"name":"stdout","text":"Warning: No image found for 5744\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 667/667 [4:54:22<00:00, 26.48s/it]  ","output_type":"stream"},{"name":"stdout","text":"Descriptions saved - LLaVADescriptionsIndofashion.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8}]}